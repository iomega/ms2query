{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in dataset\n",
    "A dataset was created in Create_dataset_for_training_MS2Query_model.ipynb. This dataset contains 4 dataframes with the scores for the top 2000 matches for each query spectrum in a validation and a training set. The scores are pasted after each other, so it can be used directly for training MS2Query models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ms2query.utils import load_pickled_file\n",
    "training_scores, training_labels, validation_scores, validation_labels = \\\n",
    "    load_pickled_file(\"../downloads/gnps_210409/train_ms2query_model/ms2q_training_data.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "It is important to use a dataset that trains recognizing high scores as well as recognizing low scores. Here the top 2000 highest MS2deepscore scores are selected. However since most tanimoto scores are selected in this way, the network is still trained mostly for low tanimoto scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 455083., 1079553., 1359605., 1151687.,  758485.,  567056.,\n",
       "         263886.,  219469.,  269733.,  325443.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def select_less_top_spectra(nr_of_spectra_cut_off, dataframe_with_tanimoto_scores, dataframe_with_scores):\n",
    "    nr_of_matches_per_training_spectra = 2000\n",
    "    \n",
    "    tanimoto_df_per_spectrum = []\n",
    "    scores_df_per_spectrum = []\n",
    "    for i in range(0, len(dataframe_with_scores), nr_of_matches_per_training_spectra):\n",
    "        scores_for_one_spectrum = dataframe_with_scores.iloc[i:i+nr_of_spectra_cut_off]\n",
    "        tanimoto_scores_one_spectrum = dataframe_with_tanimoto_scores.iloc[i:i+nr_of_spectra_cut_off]\n",
    "        tanimoto_df_per_spectrum.append(tanimoto_scores_one_spectrum)\n",
    "        scores_df_per_spectrum.append(scores_for_one_spectrum)\n",
    "        \n",
    "    selected_tanimoto_scores = pd.concat(tanimoto_df_per_spectrum)\n",
    "    selected_scores = pd.concat(scores_df_per_spectrum)\n",
    "    \n",
    "    return selected_tanimoto_scores, selected_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the top 100\n",
    "The top 100 spectra are selected, to get a bit better distribution of tanimoto scores in training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20310., 46306., 59612., 51393., 35768., 27296., 15609., 15437.,\n",
       "        18540., 32229.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_labels_top_100, training_scores_top_100 = select_less_top_spectra(100, training_labels, training_scores)\n",
    "validation_labels_top_100, validation_scores_top_100 = select_less_top_spectra(100, validation_labels, validation_scores)\n",
    "\n",
    "plt.hist(training_labels_top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MS2Query model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with all score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20157/20157 [==============================] - 24s 1ms/step - loss: 0.0342 - mae: 0.1426 - val_loss: 0.0299 - val_mae: 0.1324\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02989, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 2/100\n",
      "20157/20157 [==============================] - 13s 650us/step - loss: 0.0293 - mae: 0.1294 - val_loss: 0.0291 - val_mae: 0.1297\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02989 to 0.02908, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 3/100\n",
      "20157/20157 [==============================] - 13s 646us/step - loss: 0.0284 - mae: 0.1267 - val_loss: 0.0283 - val_mae: 0.1272\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02908 to 0.02827, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 4/100\n",
      "20157/20157 [==============================] - 13s 644us/step - loss: 0.0281 - mae: 0.1259 - val_loss: 0.0297 - val_mae: 0.1310\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02827\n",
      "Epoch 5/100\n",
      "20157/20157 [==============================] - 18s 871us/step - loss: 0.0276 - mae: 0.1251 - val_loss: 0.0281 - val_mae: 0.1259\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02827 to 0.02815, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 6/100\n",
      "20157/20157 [==============================] - 36s 2ms/step - loss: 0.0274 - mae: 0.1244 - val_loss: 0.0279 - val_mae: 0.1258\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02815 to 0.02791, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 7/100\n",
      "20157/20157 [==============================] - 13s 661us/step - loss: 0.0272 - mae: 0.1238 - val_loss: 0.0279 - val_mae: 0.1256\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02791\n",
      "Epoch 8/100\n",
      "20157/20157 [==============================] - 14s 692us/step - loss: 0.0270 - mae: 0.1233 - val_loss: 0.0281 - val_mae: 0.1273\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02791\n",
      "Epoch 9/100\n",
      "20157/20157 [==============================] - 13s 648us/step - loss: 0.0271 - mae: 0.1234 - val_loss: 0.0275 - val_mae: 0.1241\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02791 to 0.02746, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 10/100\n",
      "20157/20157 [==============================] - 27s 1ms/step - loss: 0.0266 - mae: 0.1226 - val_loss: 0.0285 - val_mae: 0.1280\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02746\n",
      "Epoch 11/100\n",
      "20157/20157 [==============================] - 41s 2ms/step - loss: 0.0263 - mae: 0.1217 - val_loss: 0.0274 - val_mae: 0.1244\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02746 to 0.02745, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 12/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0262 - mae: 0.1213 - val_loss: 0.0272 - val_mae: 0.1242\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02745 to 0.02725, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 13/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0258 - mae: 0.1204 - val_loss: 0.0276 - val_mae: 0.1236\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02725\n",
      "Epoch 14/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0258 - mae: 0.1203 - val_loss: 0.0271 - val_mae: 0.1245\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02725 to 0.02705, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 15/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0258 - mae: 0.1202 - val_loss: 0.0279 - val_mae: 0.1259\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02705\n",
      "Epoch 16/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0255 - mae: 0.1196 - val_loss: 0.0265 - val_mae: 0.1233\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02705 to 0.02649, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 17/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0255 - mae: 0.1193 - val_loss: 0.0268 - val_mae: 0.1232\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02649\n",
      "Epoch 18/100\n",
      "20157/20157 [==============================] - 40s 2ms/step - loss: 0.0254 - mae: 0.1193 - val_loss: 0.0266 - val_mae: 0.1218\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02649\n",
      "Epoch 19/100\n",
      "20157/20157 [==============================] - 21s 1ms/step - loss: 0.0254 - mae: 0.1192 - val_loss: 0.0265 - val_mae: 0.1220\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02649\n",
      "Epoch 20/100\n",
      "20157/20157 [==============================] - 16s 817us/step - loss: 0.0253 - mae: 0.1187 - val_loss: 0.0264 - val_mae: 0.1227\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02649 to 0.02641, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 21/100\n",
      "20157/20157 [==============================] - 13s 663us/step - loss: 0.0252 - mae: 0.1186 - val_loss: 0.0270 - val_mae: 0.1227\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02641\n",
      "Epoch 22/100\n",
      "20157/20157 [==============================] - 13s 659us/step - loss: 0.0252 - mae: 0.1185 - val_loss: 0.0264 - val_mae: 0.1228\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02641 to 0.02639, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 23/100\n",
      "20157/20157 [==============================] - 13s 652us/step - loss: 0.0250 - mae: 0.1182 - val_loss: 0.0269 - val_mae: 0.1246\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02639\n",
      "Epoch 24/100\n",
      "20157/20157 [==============================] - 13s 659us/step - loss: 0.0250 - mae: 0.1183 - val_loss: 0.0259 - val_mae: 0.1212\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02639 to 0.02595, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 25/100\n",
      "20157/20157 [==============================] - 14s 688us/step - loss: 0.0250 - mae: 0.1182 - val_loss: 0.0261 - val_mae: 0.1219\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02595\n",
      "Epoch 26/100\n",
      "20157/20157 [==============================] - 14s 677us/step - loss: 0.0250 - mae: 0.1180 - val_loss: 0.0263 - val_mae: 0.1215\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02595\n",
      "Epoch 27/100\n",
      "20157/20157 [==============================] - 14s 679us/step - loss: 0.0248 - mae: 0.1177 - val_loss: 0.0265 - val_mae: 0.1221\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02595\n",
      "Epoch 28/100\n",
      "20157/20157 [==============================] - 20s 1ms/step - loss: 0.0247 - mae: 0.1177 - val_loss: 0.0264 - val_mae: 0.1203\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02595\n",
      "Epoch 29/100\n",
      "20157/20157 [==============================] - 16s 811us/step - loss: 0.0247 - mae: 0.1174 - val_loss: 0.0262 - val_mae: 0.1222\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02595\n",
      "Epoch 30/100\n",
      "20157/20157 [==============================] - 13s 656us/step - loss: 0.0246 - mae: 0.1173 - val_loss: 0.0259 - val_mae: 0.1207\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02595 to 0.02586, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 31/100\n",
      "20157/20157 [==============================] - 13s 655us/step - loss: 0.0247 - mae: 0.1173 - val_loss: 0.0259 - val_mae: 0.1199\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02586\n",
      "Epoch 32/100\n",
      "20157/20157 [==============================] - 24s 1ms/step - loss: 0.0248 - mae: 0.1174 - val_loss: 0.0259 - val_mae: 0.1208\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02586\n",
      "Epoch 33/100\n",
      "20157/20157 [==============================] - 28s 1ms/step - loss: 0.0246 - mae: 0.1170 - val_loss: 0.0276 - val_mae: 0.1255\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02586\n",
      "Epoch 34/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0246 - mae: 0.1170 - val_loss: 0.0270 - val_mae: 0.1232\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02586\n",
      "Epoch 35/100\n",
      "20157/20157 [==============================] - 13s 660us/step - loss: 0.0244 - mae: 0.1167 - val_loss: 0.0263 - val_mae: 0.1216\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02586\n",
      "Epoch 36/100\n",
      "20157/20157 [==============================] - 13s 649us/step - loss: 0.0243 - mae: 0.1163 - val_loss: 0.0263 - val_mae: 0.1213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02586\n",
      "Epoch 37/100\n",
      "20157/20157 [==============================] - 13s 641us/step - loss: 0.0244 - mae: 0.1162 - val_loss: 0.0260 - val_mae: 0.1199\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02586\n",
      "Epoch 38/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0243 - mae: 0.1163 - val_loss: 0.0269 - val_mae: 0.1224\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.02586\n",
      "Epoch 39/100\n",
      "20157/20157 [==============================] - 13s 656us/step - loss: 0.0243 - mae: 0.1161 - val_loss: 0.0259 - val_mae: 0.1204\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.02586\n",
      "Epoch 40/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0243 - mae: 0.1162 - val_loss: 0.0258 - val_mae: 0.1220\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.02586 to 0.02582, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 41/100\n",
      "20157/20157 [==============================] - 14s 684us/step - loss: 0.0243 - mae: 0.1162 - val_loss: 0.0264 - val_mae: 0.1222\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.02582\n",
      "Epoch 42/100\n",
      "20157/20157 [==============================] - 13s 667us/step - loss: 0.0244 - mae: 0.1164 - val_loss: 0.0262 - val_mae: 0.1216\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.02582\n",
      "Epoch 43/100\n",
      "20157/20157 [==============================] - 14s 691us/step - loss: 0.0241 - mae: 0.1158 - val_loss: 0.0268 - val_mae: 0.1229\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.02582\n",
      "Epoch 44/100\n",
      "20157/20157 [==============================] - 13s 640us/step - loss: 0.0242 - mae: 0.1159 - val_loss: 0.0263 - val_mae: 0.1217\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.02582\n",
      "Epoch 45/100\n",
      "20157/20157 [==============================] - 13s 663us/step - loss: 0.0241 - mae: 0.1160 - val_loss: 0.0258 - val_mae: 0.1203\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.02582 to 0.02577, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 46/100\n",
      "20157/20157 [==============================] - 13s 668us/step - loss: 0.0240 - mae: 0.1156 - val_loss: 0.0265 - val_mae: 0.1230\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.02577\n",
      "Epoch 47/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0242 - mae: 0.1160 - val_loss: 0.0264 - val_mae: 0.1208\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.02577\n",
      "Epoch 48/100\n",
      "20157/20157 [==============================] - 13s 655us/step - loss: 0.0241 - mae: 0.1156 - val_loss: 0.0258 - val_mae: 0.1197\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.02577\n",
      "Epoch 49/100\n",
      "20157/20157 [==============================] - 13s 656us/step - loss: 0.0240 - mae: 0.1155 - val_loss: 0.0260 - val_mae: 0.1210\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.02577\n",
      "Epoch 50/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0241 - mae: 0.1157 - val_loss: 0.0271 - val_mae: 0.1225\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.02577\n",
      "Epoch 51/100\n",
      "20157/20157 [==============================] - 13s 660us/step - loss: 0.0239 - mae: 0.1152 - val_loss: 0.0260 - val_mae: 0.1193\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.02577\n",
      "Epoch 52/100\n",
      "20157/20157 [==============================] - 13s 656us/step - loss: 0.0239 - mae: 0.1153 - val_loss: 0.0259 - val_mae: 0.1198\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.02577\n",
      "Epoch 53/100\n",
      "20157/20157 [==============================] - 13s 659us/step - loss: 0.0241 - mae: 0.1154 - val_loss: 0.0259 - val_mae: 0.1196\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.02577\n",
      "Epoch 54/100\n",
      "20157/20157 [==============================] - 13s 654us/step - loss: 0.0239 - mae: 0.1153 - val_loss: 0.0257 - val_mae: 0.1191\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.02577 to 0.02575, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 55/100\n",
      "20157/20157 [==============================] - 13s 662us/step - loss: 0.0239 - mae: 0.1150 - val_loss: 0.0259 - val_mae: 0.1203\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.02575\n",
      "Epoch 56/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0240 - mae: 0.1153 - val_loss: 0.0261 - val_mae: 0.1215\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.02575\n",
      "Epoch 57/100\n",
      "20157/20157 [==============================] - 13s 659us/step - loss: 0.0239 - mae: 0.1150 - val_loss: 0.0256 - val_mae: 0.1200\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.02575 to 0.02565, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 58/100\n",
      "20157/20157 [==============================] - 13s 663us/step - loss: 0.0238 - mae: 0.1148 - val_loss: 0.0261 - val_mae: 0.1218\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.02565\n",
      "Epoch 59/100\n",
      "20157/20157 [==============================] - 13s 661us/step - loss: 0.0239 - mae: 0.1150 - val_loss: 0.0255 - val_mae: 0.1190\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.02565 to 0.02554, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 60/100\n",
      "20157/20157 [==============================] - 13s 660us/step - loss: 0.0238 - mae: 0.1149 - val_loss: 0.0259 - val_mae: 0.1212\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.02554\n",
      "Epoch 61/100\n",
      "20157/20157 [==============================] - 13s 658us/step - loss: 0.0239 - mae: 0.1152 - val_loss: 0.0256 - val_mae: 0.1202\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.02554\n",
      "Epoch 62/100\n",
      "20157/20157 [==============================] - 14s 672us/step - loss: 0.0237 - mae: 0.1148 - val_loss: 0.0253 - val_mae: 0.1187\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.02554 to 0.02530, saving model to ../downloads/gnps_210409/train_ms2query_model\\ms2query_model_all_scores.hdf5\n",
      "Epoch 63/100\n",
      "20157/20157 [==============================] - 14s 685us/step - loss: 0.0239 - mae: 0.1152 - val_loss: 0.0265 - val_mae: 0.1235\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02530\n",
      "Epoch 64/100\n",
      "20157/20157 [==============================] - 13s 664us/step - loss: 0.0238 - mae: 0.1149 - val_loss: 0.0257 - val_mae: 0.1199\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.02530\n",
      "Epoch 65/100\n",
      "20157/20157 [==============================] - 13s 661us/step - loss: 0.0238 - mae: 0.1148 - val_loss: 0.0258 - val_mae: 0.1213\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02530\n",
      "Epoch 66/100\n",
      "20157/20157 [==============================] - 13s 657us/step - loss: 0.0237 - mae: 0.1148 - val_loss: 0.0259 - val_mae: 0.1208\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.02530\n",
      "Epoch 67/100\n",
      "20157/20157 [==============================] - 14s 671us/step - loss: 0.0237 - mae: 0.1145 - val_loss: 0.0263 - val_mae: 0.1206\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.02530\n",
      "Epoch 68/100\n",
      "20157/20157 [==============================] - 13s 623us/step - loss: 0.0238 - mae: 0.1148 - val_loss: 0.0259 - val_mae: 0.1212\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.02530\n",
      "Epoch 69/100\n",
      "20157/20157 [==============================] - 13s 621us/step - loss: 0.0236 - mae: 0.1144 - val_loss: 0.0258 - val_mae: 0.1194\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.02530\n",
      "Epoch 70/100\n",
      "20157/20157 [==============================] - 13s 641us/step - loss: 0.0236 - mae: 0.1142 - val_loss: 0.0258 - val_mae: 0.1206\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.02530\n",
      "Epoch 71/100\n",
      "20157/20157 [==============================] - 2582s 128ms/step - loss: 0.0236 - mae: 0.1145 - val_loss: 0.0257 - val_mae: 0.1195\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.02530\n",
      "Epoch 72/100\n",
      "20157/20157 [==============================] - 31s 2ms/step - loss: 0.0234 - mae: 0.1139 - val_loss: 0.0257 - val_mae: 0.1191\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.02530\n",
      "Epoch 00072: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.sequential.Sequential at 0x2362fc31820>,\n",
       " {'loss': [0.031496305018663406,\n",
       "   0.02897644229233265,\n",
       "   0.028264440596103668,\n",
       "   0.027927471324801445,\n",
       "   0.027639517560601234,\n",
       "   0.027369525283575058,\n",
       "   0.02715250663459301,\n",
       "   0.027000227943062782,\n",
       "   0.02679959498345852,\n",
       "   0.02651801146566868,\n",
       "   0.02633279375731945,\n",
       "   0.02611823007464409,\n",
       "   0.025944558903574944,\n",
       "   0.025843774899840355,\n",
       "   0.025710176676511765,\n",
       "   0.025606919080018997,\n",
       "   0.025482570752501488,\n",
       "   0.025425130501389503,\n",
       "   0.025325879454612732,\n",
       "   0.025274762883782387,\n",
       "   0.02519717626273632,\n",
       "   0.025126438587903976,\n",
       "   0.025033285841345787,\n",
       "   0.02500300481915474,\n",
       "   0.024940233677625656,\n",
       "   0.02493630349636078,\n",
       "   0.02482735365629196,\n",
       "   0.024790585041046143,\n",
       "   0.02474038675427437,\n",
       "   0.024725740775465965,\n",
       "   0.024685680866241455,\n",
       "   0.024652373045682907,\n",
       "   0.024575894698500633,\n",
       "   0.02452464960515499,\n",
       "   0.024465490132570267,\n",
       "   0.024401040747761726,\n",
       "   0.0243966244161129,\n",
       "   0.02437642216682434,\n",
       "   0.024340301752090454,\n",
       "   0.024313507601618767,\n",
       "   0.024278299883008003,\n",
       "   0.02428564615547657,\n",
       "   0.024223659187555313,\n",
       "   0.0242041926831007,\n",
       "   0.024174092337489128,\n",
       "   0.02414625696837902,\n",
       "   0.024127565324306488,\n",
       "   0.024094510823488235,\n",
       "   0.02405346930027008,\n",
       "   0.024025527760386467,\n",
       "   0.024048171937465668,\n",
       "   0.024024052545428276,\n",
       "   0.023979760706424713,\n",
       "   0.023949002847075462,\n",
       "   0.023932650685310364,\n",
       "   0.023924313485622406,\n",
       "   0.02389623038470745,\n",
       "   0.023887325078248978,\n",
       "   0.023864110931754112,\n",
       "   0.023863889276981354,\n",
       "   0.023836985230445862,\n",
       "   0.02379610203206539,\n",
       "   0.02379489876329899,\n",
       "   0.023798657581210136,\n",
       "   0.023765217512845993,\n",
       "   0.023766756057739258,\n",
       "   0.023728234693408012,\n",
       "   0.02374897710978985,\n",
       "   0.023736122995615005,\n",
       "   0.023701630532741547,\n",
       "   0.023681459948420525,\n",
       "   0.023657960817217827],\n",
       "  'mae': [0.1358470320701599,\n",
       "   0.12847241759300232,\n",
       "   0.12654975056648254,\n",
       "   0.1256580650806427,\n",
       "   0.12496775388717651,\n",
       "   0.12427689135074615,\n",
       "   0.12374123185873032,\n",
       "   0.12336235493421555,\n",
       "   0.1228037178516388,\n",
       "   0.12228900194168091,\n",
       "   0.12176749110221863,\n",
       "   0.12119466066360474,\n",
       "   0.12081129103899002,\n",
       "   0.12052839249372482,\n",
       "   0.12007106095552444,\n",
       "   0.11983852833509445,\n",
       "   0.11942815035581589,\n",
       "   0.11930907517671585,\n",
       "   0.11908388882875443,\n",
       "   0.11886661499738693,\n",
       "   0.11873617768287659,\n",
       "   0.11852822452783585,\n",
       "   0.11834146082401276,\n",
       "   0.11819718778133392,\n",
       "   0.11800077557563782,\n",
       "   0.11794521659612656,\n",
       "   0.11776989698410034,\n",
       "   0.11771001666784286,\n",
       "   0.1174626350402832,\n",
       "   0.11752594262361526,\n",
       "   0.11733099818229675,\n",
       "   0.11727598309516907,\n",
       "   0.11698075383901596,\n",
       "   0.11688833683729172,\n",
       "   0.11669477075338364,\n",
       "   0.11657370626926422,\n",
       "   0.116478331387043,\n",
       "   0.11645880341529846,\n",
       "   0.11631154268980026,\n",
       "   0.11628442257642746,\n",
       "   0.1161881536245346,\n",
       "   0.11622755229473114,\n",
       "   0.11602765321731567,\n",
       "   0.11599221080541611,\n",
       "   0.11593082547187805,\n",
       "   0.11583993583917618,\n",
       "   0.11584968864917755,\n",
       "   0.11567381024360657,\n",
       "   0.11559410393238068,\n",
       "   0.11555736511945724,\n",
       "   0.11553754657506943,\n",
       "   0.11546411365270615,\n",
       "   0.11540215462446213,\n",
       "   0.11524558812379837,\n",
       "   0.11519672721624374,\n",
       "   0.11518226563930511,\n",
       "   0.11513862758874893,\n",
       "   0.1150587797164917,\n",
       "   0.11502987891435623,\n",
       "   0.11499415338039398,\n",
       "   0.11496394127607346,\n",
       "   0.114919513463974,\n",
       "   0.11479440331459045,\n",
       "   0.11488109081983566,\n",
       "   0.11470059305429459,\n",
       "   0.11476395279169083,\n",
       "   0.11466344445943832,\n",
       "   0.11466783285140991,\n",
       "   0.11467635631561279,\n",
       "   0.11456628888845444,\n",
       "   0.11446227133274078,\n",
       "   0.11447464674711227],\n",
       "  'val_loss': [0.02988933026790619,\n",
       "   0.029079139232635498,\n",
       "   0.02827388420701027,\n",
       "   0.02972024492919445,\n",
       "   0.028149524703621864,\n",
       "   0.027910050004720688,\n",
       "   0.02794407494366169,\n",
       "   0.028118491172790527,\n",
       "   0.027459394186735153,\n",
       "   0.028513941913843155,\n",
       "   0.027446258813142776,\n",
       "   0.02724965289235115,\n",
       "   0.027646014466881752,\n",
       "   0.027054762467741966,\n",
       "   0.027947213500738144,\n",
       "   0.02648669295012951,\n",
       "   0.026806749403476715,\n",
       "   0.026587912812829018,\n",
       "   0.026492228731513023,\n",
       "   0.026414979249238968,\n",
       "   0.027028165757656097,\n",
       "   0.026389105245471,\n",
       "   0.026850614696741104,\n",
       "   0.02594519406557083,\n",
       "   0.026125675067305565,\n",
       "   0.02627815306186676,\n",
       "   0.026451582089066505,\n",
       "   0.026438230648636818,\n",
       "   0.026165775954723358,\n",
       "   0.02586473524570465,\n",
       "   0.025893177837133408,\n",
       "   0.025897040963172913,\n",
       "   0.027648720890283585,\n",
       "   0.02702592872083187,\n",
       "   0.026341207325458527,\n",
       "   0.026303451508283615,\n",
       "   0.02600991167128086,\n",
       "   0.02688414230942726,\n",
       "   0.02587340585887432,\n",
       "   0.02582445926964283,\n",
       "   0.026437558233737946,\n",
       "   0.026225849986076355,\n",
       "   0.026778725907206535,\n",
       "   0.026275472715497017,\n",
       "   0.02577034942805767,\n",
       "   0.026503879576921463,\n",
       "   0.026394298300147057,\n",
       "   0.0258481428027153,\n",
       "   0.025963641703128815,\n",
       "   0.027117513120174408,\n",
       "   0.02604786306619644,\n",
       "   0.02587887831032276,\n",
       "   0.02585006132721901,\n",
       "   0.02574613317847252,\n",
       "   0.02589523419737816,\n",
       "   0.026145651936531067,\n",
       "   0.025648627430200577,\n",
       "   0.026102114468812943,\n",
       "   0.025540167465806007,\n",
       "   0.025939177721738815,\n",
       "   0.025645526126027107,\n",
       "   0.025301076471805573,\n",
       "   0.026511969044804573,\n",
       "   0.025745101273059845,\n",
       "   0.025828566402196884,\n",
       "   0.025913773104548454,\n",
       "   0.026267772540450096,\n",
       "   0.025895627215504646,\n",
       "   0.02581493929028511,\n",
       "   0.02582385018467903,\n",
       "   0.025735432282090187,\n",
       "   0.025707246735692024],\n",
       "  'val_mae': [0.13243761658668518,\n",
       "   0.12970378994941711,\n",
       "   0.1271675080060959,\n",
       "   0.13101905584335327,\n",
       "   0.125921830534935,\n",
       "   0.12578769028186798,\n",
       "   0.12563206255435944,\n",
       "   0.12734949588775635,\n",
       "   0.12410655617713928,\n",
       "   0.12798504531383514,\n",
       "   0.12435631453990936,\n",
       "   0.1242111325263977,\n",
       "   0.1236070767045021,\n",
       "   0.12446720153093338,\n",
       "   0.12589994072914124,\n",
       "   0.12333016097545624,\n",
       "   0.12315676361322403,\n",
       "   0.12184741348028183,\n",
       "   0.12196815758943558,\n",
       "   0.1226731389760971,\n",
       "   0.12267391383647919,\n",
       "   0.12283484637737274,\n",
       "   0.12464908510446548,\n",
       "   0.12123186886310577,\n",
       "   0.12187568098306656,\n",
       "   0.1215093582868576,\n",
       "   0.12211859226226807,\n",
       "   0.12031293660402298,\n",
       "   0.12216129153966904,\n",
       "   0.12066952884197235,\n",
       "   0.11994083970785141,\n",
       "   0.1207565888762474,\n",
       "   0.12545745074748993,\n",
       "   0.12316006422042847,\n",
       "   0.12156953662633896,\n",
       "   0.12126938253641129,\n",
       "   0.1198926493525505,\n",
       "   0.12235882878303528,\n",
       "   0.12038694322109222,\n",
       "   0.12196293473243713,\n",
       "   0.1222454383969307,\n",
       "   0.12161757797002792,\n",
       "   0.12289100140333176,\n",
       "   0.12166613340377808,\n",
       "   0.12029053270816803,\n",
       "   0.12298166751861572,\n",
       "   0.12084931880235672,\n",
       "   0.11965008080005646,\n",
       "   0.12104766815900803,\n",
       "   0.12247446924448013,\n",
       "   0.1192760244011879,\n",
       "   0.11981850117444992,\n",
       "   0.1195853129029274,\n",
       "   0.11911781877279282,\n",
       "   0.12032608687877655,\n",
       "   0.12149093300104141,\n",
       "   0.1200103908777237,\n",
       "   0.12180750072002411,\n",
       "   0.11903747916221619,\n",
       "   0.1211991086602211,\n",
       "   0.12018249183893204,\n",
       "   0.11873600631952286,\n",
       "   0.12352060526609421,\n",
       "   0.11987093091011047,\n",
       "   0.1212620735168457,\n",
       "   0.12075349688529968,\n",
       "   0.12063143402338028,\n",
       "   0.1211799755692482,\n",
       "   0.11938410997390747,\n",
       "   0.12061362713575363,\n",
       "   0.1194971427321434,\n",
       "   0.11908403784036636]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ms2query.train_ms2query_nn import create_and_train_ms2query_nn\n",
    "create_and_train_ms2query_nn(training_scores_top_100, training_labels_top_100, \\\n",
    "                             validation_scores_top_100, validation_labels_top_100, [48,48,1],\\\n",
    "                             save_name = \"../downloads/gnps_210409/train_ms2query_model/ms2query_model_all_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
